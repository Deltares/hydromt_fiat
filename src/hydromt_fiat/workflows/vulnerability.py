"""Vulnerability workflows."""

import logging

import pandas as pd
from barril.units import Scalar

from hydromt_fiat.utils import create_query, standard_unit

__all__ = ["vulnerability_curves"]

logger = logging.getLogger(f"hydromt.{__name__}")


def vulnerability_curves(
    vulnerability_data: pd.DataFrame,
    vulnerability_linking: pd.DataFrame | None = None,
    *,
    unit: str = "m",
    index_name: str = "water depth",
    **select: dict,
) -> tuple[pd.DataFrame]:
    """Create vulnerability curves from raw data.

    Parameters
    ----------
    vulnerability_data : pd.DataFrame
        The raw vulnerability dataset.
    vulnerability_linking : pd.DataFrame | None, optional
        The vulnerability linking table, by default None
    unit : str, optional
        The unit of the vulnerability dataset index, by default "m"
    index_name : str, optional
        The name of the outgoing vulnerability curves dataset index,
        by default "water depth"
    select : dict, optional
        Keyword arguments to select data from `vulnerability_data`

    Returns
    -------
    tuple[pd.DataFrame]
        A tuple containing the the vulnerability curves and updated link table.
    """
    # Quick check on the data
    if "curve" not in vulnerability_data:
        raise KeyError("The 'curve' column in not present in the vulnerability data")
    # Sort the linking table
    if vulnerability_linking is None:
        logger.info("No linking table provided, inferred from raw vulnerability data")
        vulnerability_linking = pd.DataFrame(
            data={
                "exposure_link": vulnerability_data["curve"].values,
                "curve": vulnerability_data["curve"].values,
            }
        )
    if "curve" not in vulnerability_linking:
        raise KeyError("The 'curve' column in not present in the linking table")
    if "exposure_type" not in vulnerability_linking:  # default to damage
        vulnerability_linking["exposure_type"] = "damage"
    # Build a query from the index kwargs
    if len(select) != 0:
        query = create_query(**select)
        vulnerability_data = vulnerability_data.query(query)

    # Query the linking data
    vulnerability_linking.loc[:, "curve_id"] = vulnerability_linking["curve"]
    types = vulnerability_data.curve.values.tolist()
    vulnerability_linking = vulnerability_linking.query(f"curve in {str(types)}")

    # Sort the sub typing of the objects
    if "subtype" in vulnerability_linking.columns:
        vulnerability_linking.loc[:, "curve_id"] = (
            vulnerability_linking["curve"] + "_" + vulnerability_linking["subtype"]
        )
    vulnerability_linking = vulnerability_linking.drop_duplicates(subset="curve_id")
    on = "curve"
    if "subtype" in vulnerability_data.columns:
        on = ["curve", "subtype"]
    vulnerability_data = pd.merge(
        vulnerability_data,
        vulnerability_linking,
        on=on,
        how="inner",
        validate="many_to_many",
    )

    # Reshape the vulnerability data
    columns = list(set(list(select.keys()) + vulnerability_linking.columns.to_list()))
    columns.remove("curve_id")
    vulnerability_data = vulnerability_data.drop(columns, axis=1)
    vulnerability_data = vulnerability_data.transpose()
    vulnerability_data = vulnerability_data.rename(
        columns=vulnerability_data.loc["curve_id"]
    )
    vulnerability_data = vulnerability_data.drop("curve_id")
    vulnerability_data.index.name = index_name

    # Again query the linking table based on the vulnerability curves
    # But this time on the curve ID
    types = vulnerability_data.columns.tolist()
    vulnerability_linking = vulnerability_linking.query(f"curve_id in {str(types)}")

    # At last reset the index
    vulnerability_data.reset_index(inplace=True)
    vulnerability_data = vulnerability_data.astype(float)

    # Scale the data according to the unit
    conversion = standard_unit(Scalar(1.0, unit))
    vulnerability_data[index_name] *= conversion.value

    return vulnerability_data, vulnerability_linking
