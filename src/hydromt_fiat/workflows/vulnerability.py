"""Vulnerability workflows."""

import logging

import pandas as pd
from barril.units import Scalar

from hydromt_fiat.utils import create_query

__all__ = ["vulnerability_curves"]

logger = logging.getLogger(f"hydromt.{__name__}")


def vulnerability_curves(
    vuln_data: pd.DataFrame,
    vuln_linking: pd.DataFrame | None = None,
    *,
    unit: str = "m",
    index_name: str = "water depth",
    **select: dict,
) -> tuple[pd.DataFrame]:
    """Create vulnerability curves from raw data.

    Parameters
    ----------
    vuln_data : pd.DataFrame
        The raw vulnerability dataset.
    vuln_linking : pd.DataFrame | None, optional
        The vulnerability linking table, by default None
    unit : str, optional
        The unit of the vulnerability dataset index, by default "m"
    index_name : str, optional
        The name of the outgoing vulnerability curves dataset index,
        by default "water depth"
    select : dict, optional
        Keyword arguments to select data from `vuln_data`

    Returns
    -------
    tuple[pd.DataFrame]
        A tuple containing the the vulnerability curves and updated link table.
    """
    # Quick check on the data
    if "type" not in vuln_data:
        raise KeyError("The 'type' column in not present in the vulnerability data")
    # Sort the linking table
    if vuln_linking is None:
        logger.info("No linking table provided, inferred from raw vulnerability data")
        vuln_linking = pd.DataFrame(
            data={
                "link": vuln_data["type"].values,
                "type": vuln_data["type"].values,
            }
        )
    if "type" not in vuln_linking:
        raise KeyError("The 'type' column in not present in the linking table")
    if "exposure_type" not in vuln_linking:  # default to damage
        vuln_linking["exposure_type"] = "damage"
    # Build a query from the index kwargs
    if len(select) != 0:
        query = create_query(**select)
        vuln_data = vuln_data.query(query)

    # Query the linking data
    vuln_linking.loc[:, "curve_id"] = vuln_linking["type"]
    types = vuln_data.type.values.tolist()
    vuln_linking = vuln_linking.query(f"type in {str(types)}")

    # Sort the sub typing of the objects
    if "subtype" in vuln_linking.columns:
        vuln_linking.loc[:, "curve_id"] = (
            vuln_linking["type"] + "_" + vuln_linking["subtype"]
        )
    vuln_linking = vuln_linking.drop_duplicates(subset="curve_id")
    on = "type"
    if "subtype" in vuln_data.columns:
        vuln_data.loc[:, "curve_id"] = vuln_data["type"] + "_" + vuln_data["subtype"]
        on = "curve_id"
    vuln_data = pd.merge(
        vuln_data,
        vuln_linking,
        on=on,
        how="inner",
        validate="many_to_many",
    )

    # Reshape the vulnerability data
    columns = list(set(list(select.keys()) + vuln_linking.columns.to_list()))
    columns.remove("curve_id")
    vuln_data = vuln_data.drop(columns, axis=1)
    vuln_data = vuln_data.transpose()
    vuln_data = vuln_data.rename(columns=vuln_data.loc["curve_id"])
    vuln_data = vuln_data.drop("curve_id")
    vuln_data.index.name = index_name

    # Again query the linking table based on the vulnerability curves
    # But this time on the curve ID
    types = vuln_data.columns.tolist()
    vuln_linking = vuln_linking.query(f"curve_id in {str(types)}")

    # At last reset the index
    vuln_data.reset_index(inplace=True)
    vuln_data = vuln_data.astype(float)

    # Scale the data according to the unit
    default_unit = Scalar(1, "m")
    arg_unit = Scalar(1, unit)
    conversion = arg_unit / default_unit
    vuln_data[index_name] *= conversion.value

    return vuln_data, vuln_linking
