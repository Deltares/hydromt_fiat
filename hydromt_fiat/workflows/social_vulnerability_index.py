from census import Census  # install in your environment using pip install Census
from us import states  # install in your environment using pip install us
from hydromt.data_catalog import DataCatalog
from logging import Logger
import pandas as pd
import numpy as np
import geopandas as gpd
from urllib.request import urlopen
from io import BytesIO
from zipfile import ZipFile
from pathlib import Path
from itertools import zip_longest
from typing import List
import shutil


def list_of_states(inverted: bool = True):
    states = {
        "AK": "Alaska",
        "AL": "Alabama",
        "AR": "Arkansas",
        "AZ": "Arizona",
        "CA": "California",
        "CO": "Colorado",
        "CT": "Connecticut",
        "DC": "District of Columbia",
        "DE": "Delaware",
        "FL": "Florida",
        "GA": "Georgia",
        "HI": "Hawaii",
        "IA": "Iowa",
        "ID": "Idaho",
        "IL": "Illinois",
        "IN": "Indiana",
        "KS": "Kansas",
        "KY": "Kentucky",
        "LA": "Louisiana",
        "MA": "Massachusetts",
        "MD": "Maryland",
        "ME": "Maine",
        "MI": "Michigan",
        "MN": "Minnesota",
        "MO": "Missouri",
        "MS": "Mississippi",
        "MT": "Montana",
        "NC": "North Carolina",
        "ND": "North Dakota",
        "NE": "Nebraska",
        "NH": "New Hampshire",
        "NJ": "New Jersey",
        "NM": "New Mexico",
        "NV": "Nevada",
        "NY": "New York",
        "OH": "Ohio",
        "OK": "Oklahoma",
        "OR": "Oregon",
        "PA": "Pennsylvania",
        "RI": "Rhode Island",
        "SC": "South Carolina",
        "SD": "South Dakota",
        "TN": "Tennessee",
        "TX": "Texas",
        "UT": "Utah",
        "VA": "Virginia",
        "VT": "Vermont",
        "WA": "Washington",
        "WI": "Wisconsin",
        "WV": "West Virginia",
        "WY": "Wyoming",
    }

    if inverted:
        states_inverted = {value: key for key, value in states.items()}
        return states_inverted
    else:
        return states


class SocialVulnerabilityIndex:
    def __init__(self, data_catalog: DataCatalog, logger: Logger, save_folder: str):
        self.data_catalog = data_catalog
        self.save_folder = save_folder
        self.census_key = Census
        self.download_codes = {}
        self.state_fips = []
        self.pd_census_data = pd.DataFrame()
        self.codebook = pd.DataFrame()
        self.indicator_groups = {}
        self.processed_census_data = pd.DataFrame()
        self.svi_fiat = pd.DataFrame()
        self.pd_domain_scores = pd.DataFrame()
        self.percentage = []
        self.columns_to_exclude = []
        self.pd_domain_scores = pd.DataFrame()
        self.pd_domain_scores_z = pd.DataFrame()
        self.pd_domain_scores_geo = pd.DataFrame()
        self.logger = logger
        self.svi_data_shp = gpd.GeoDataFrame()
        self.block_groups = gpd.GeoDataFrame()

    def read_dataset(self, path: str):
        """If you have your own dataset (e.g. already downloaded census data), you can use this to load it from a csv
        Make sure to only use the relevant functions in fiat then"""
        self.pd_census_data = self.data_catalog.get_dataframe(path, sep=";", header=0)
        self.logger.info(f"You loaded your dataset from {path}")

    def set_up_census_key(self, census_key: str):
        """The Census key can be inputted in the ini file.
        This is a unique key that every user needs to specify to download census data

        Parameters
        ----------
        census_key : str
            The unique key a user gets from the census download website (an API token basically)
        """

        self.census_key = Census(census_key)
        self.logger.info(
            f"your census key {census_key} is used to download data from the Census website "
        )

    def variable_code_csv_to_pd_df(self, path: str):
        """Loading the csv in and converting to pd_dataframe to be used in the other functions.

        Parameters
        ----------
        path : specify the path where the census_vulnerability_data_codebook.csv is stored.

        The Excel/CSV specifies the names of the variables that need to be downloaded from the Census API
        It contains additional information that can be called and used in the other functions.
        """
        self.codebook = self.data_catalog.get_dataframe(path)

    def set_up_download_codes(self):
        """From the codebook.csv we find the column with the Census code with 'E' (estimate) at the end.
        This are the codes that need to be inputted into the download_census_data function as the fields argument
        In this way the download_census_data function knows which variables to retrieve from the API.
        """
        self.download_codes = list(self.codebook["Census_code_withE"])
        self.download_codes.append("NAME")

    def set_up_state_code(self, state_abbreviation: List[str]):
        """download census data for a state

        Parameters
        ----------
        state_abbreviation : str
            Abbreviation of the state for which you want to set up the census data download
        """
        states_done = []
        for state in state_abbreviation:
            if state not in states_done:
                self.logger.info(
                    f"The states for which census data will be downloaded is: {list_of_states(inverted=False)[state]}"
                )
                state_obj = getattr(states, state)
                self.state_fips.append(state_obj.fips)
                states_done.append(state)

    def download_census_data(self, year_data):
        """download the census data
        it is possible to also make the county, tract and blockgroup flexible so that a user could specify exactly what to download
        But: bear in mind, with social vulneraiblity we compare areas against each other, so Ideally you would have a large enough dataset (for statistical and validity purposes)
        """
        dfs = []
        for sf in self.state_fips:
            download_census_codes = self.census_key.acs5.state_county_blockgroup(
                fields=self.download_codes,
                state_fips=sf,
                county_fips="*",
                tract="*",
                blockgroup="*",
                year=year_data,
            )
            dfs.append(pd.DataFrame(download_census_codes))

        self.pd_census_data = pd.concat(dfs, ignore_index=True)
        self.logger.info(
            "The census data was successfully downloaded using the specified variables in the codebook Excel"
        )

    def rename_census_data(self, from_name: str, to_name: str):
        """renaming the columns so that they have variable names instead of variable codes as their headers

        Parameters
        ----------
        from_name : str
            The name that you want to replace
        to_name : str
            The name to replace with
        """
        name_dict = dict(zip(self.codebook[from_name], self.codebook[to_name]))
        self.pd_census_data = self.pd_census_data.rename(columns=name_dict)

    def identify_no_data(self):
        """This function identifies and replaces negative values in the pd_census_data DataFrame with NaN.
        It then filters out rows that have no values greater than 0 in the columns (excluding specific columns) and updates the DataFrame accordingly.
        The resulting DataFrame will contain only the rows where at least one value is greater than 0 in the selected columns, with negative values replaced by NaN.
        """
        self.columns_to_exclude = [
            "GEO_ID",
            "NAME",
            "state",
            "county",
            "tract",
            "block group",
            "Index",
        ]
        rows_to_keep = []

        for column, series in self.pd_census_data.items():
            if column in self.columns_to_exclude:
                continue
            for idx, value in series.items():
                if value < 0:
                    self.pd_census_data.loc[idx, column] = np.nan

        for index, row in self.pd_census_data.iterrows():
            values_to_check = row.loc[~row.index.isin(self.columns_to_exclude)]
            if any(values_to_check > 0):
                rows_to_keep.append(index)

        self.pd_census_data = self.pd_census_data.loc[rows_to_keep]

    def check_nan_variable_columns(self, column_to_check: str, indicator_column: str):
        """
                Parameters
        ----------
        column_to_check : str
            Column that contains variable names (from the codebook)
        indicator_column : str
            Column containing the indicator names belonging to a variable (from the codebook)

        This function checks the percentage of nan values in each column
        In the Census data case, we want to ensure that a variable has enough entries to be meaningful
        We chose 5%, i.e. the variable needs to have data for at least 95% of regions
            When a census variable is dropped from the dataset because of too many nan-values (>5%) we print a logging message
        """

        nan_percentages = (
            self.pd_census_data.isna().sum() / len(self.pd_census_data)
        ) * 100
        self.percentage = []
        self.missing_values = {}

        for column in nan_percentages.index:
            if column in self.columns_to_exclude:
                continue
            percentage = nan_percentages[column]

            if percentage > 5:
                self.pd_census_data.drop(column, axis=1, inplace=True)
                drop = f"The variable '{column}' is dropped because of {percentage:.2f}% missing values (Nan)"
                self.percentage.append(drop)

            else:
                result = f"Percentage of NaN values in {column}: {percentage:.2f}%"
                self.percentage.append(result)
                self.fill_nan_columns(column)

        for value in self.codebook[column_to_check]:
            if value not in self.pd_census_data.columns:
                indicator_name = self.codebook.loc[
                    self.codebook[column_to_check] == value, indicator_column
                ].iloc[0]
                self.missing_values[value] = indicator_name
                self.logger.info(
                    f"From indicator '{indicator_name}' the variable '{value}' is dropped because of too many missing values (exceeding the threshold of 5%)"
                )

    def fill_nan_columns(self, nancolumn: str):
        """This function is created to fill no data with the column mean

        Parameters
        ----------
        nancolumn : str
            column with nans

        In the case of the census data: The column mean is the average value of all regions, for that specific variable
        """

        column_average = self.pd_census_data[nancolumn].mean()
        self.pd_census_data[nancolumn].fillna(column_average, inplace=True)

    def check_zeroes_variable_rows(self):
        # Here you count the zeroes in a row.
        # This is under the assumption that if more than half of the data is zero, it indicates missing data.
        # Note: in theory a region can be least vulnerable on all commponents and score zero everywhere. We assume however that it is unlikely that this is the case for more than 50%
        self.excluded_regions = []
        num_columns = self.pd_census_data.shape[1]
        if "NAME" in self.pd_census_data.columns:
            num_columns = self.pd_census_data.shape[1] - 1

        for index, row in self.pd_census_data.iterrows():
            num_zeros = (row == 0).sum()
            percentage_zeros = num_zeros / num_columns

            if percentage_zeros > 0.5:
                self.pd_census_data = self.pd_census_data.drop(index)
                names_exluced_areas = row["NAME"]
                self.excluded_regions.append(names_exluced_areas)

    def create_indicator_groups(
        self, column_to_group: str, aggregation_name: str
    ) -> dict:
        """With this code we can group the different variables into their indicator group
        We can also group the indicators into the domains with this code

        Parameters
        ----------
        column_to_group : str
            This is the column which you want to group together into an aggregate group.
            For example grouping all the Census variables into the indicators they belong to
        aggregation_name : str
           This is the column by which you want to group. So for example:
           this column indicates the group name for the indicators to which variables belong.

        Returns
        -------
        dict
            The dictionary matches the contents of a group to the group name (key)
        """
        translation = {}
        for indicator_code, group_df in self.codebook.groupby(aggregation_name):
            translation[indicator_code] = set(group_df[column_to_group].tolist())
        return translation

    def processing_svi_data(self, translation_var_ind: dict):
        """
        This is a function to pre-process the Census data.
        The function is quite specific to the census data! Check if it can be applied to other datasets!


        Parameters
        ----------
        translation_var_ind : dictionary
            dictionary contiaining the variable name and the indicator name that belongs to each variable

        It groups the Census variables together to create the actual indicators.
        The actual indicator groups are indicated by the Indicator_full_name and Indicator_code in the codebook.
        Most of the indicators are percentages, but there are also Median values and Per Capita values.
        The preprocessing needs to be done to convert variables into indicators, for example by dividing a variable by a total * 100 to find a percentage.
        The variables with 'total' in their name, are used to do divisions for the creation of the percentages / shares.
        Therefore, this function below firstly finds for every indicator group if there is a variable with 'total' in the name.
        If that is not the case, the indicator is not a percentage or share but a Median value.
        We can just keep the Median value and put it in the final dataframe (processed_census_data)
        When there are more than 2 columns, we first need to sum all the data (except for the total).
        Then we compute the percentage of that sum as a share of the whole (the total). For example to find the population under 5 years and over 65 years we need to add females and males and the different age groups and then take that as a percentage of the total population.
        If there are exactly two columns, then the one needs to be divided by the other. The other is always the total.
        Together these procedures make up the processed indicators.

        """

        # create an empty dataframe to store the results
        self.processed_census_data = pd.DataFrame()
        group_varslist = []
        updated_translation_var_ind = {}

        # Loop over the indicator_groups
        for key, values in translation_var_ind.items():
            updated_values = [
                value for value in values if value not in self.missing_values.keys()
            ]
            if len(updated_values) > 0:
                updated_translation_var_ind[key] = updated_values
            if key in self.columns_to_exclude:
                continue

        # Update translation_var_ind with the key-value pairs and remove the 'missing values' / the excluded values

        for key, values in updated_translation_var_ind.items():
            # extract columns that match 'values'
            for value in values:
                if value in self.pd_census_data.columns:
                    group_vars = self.pd_census_data[values]
                    group_varslist.append(group_vars)
                else:
                    continue

            # identify the columns that contain 'total' in the name. These are the columns that are used for division, to calculate percentages etc.
            total_cols = [col for col in group_vars.columns if "total" in col.lower()]

            # if there is no total column, the indicator is a single column (e.g. Median) and we just want to take that column
            if len(total_cols) == 0:
                self.processed_census_data[key] = group_vars.iloc[:, 0]

            # if there are more than 2 columns we need to first sum the columns together and then divide over the total to get a percentage
            elif len(group_vars.columns) > 2:
                # identify the columns that do not contain 'total' in the name
                value_cols = [
                    col for col in group_vars.columns if col not in total_cols
                ]
                # sum the values across the rows (axis = 1) for value columns
                sum_values = group_vars[value_cols].sum(axis=1)
                # divide the sum of values by the total column for this group
                total_col = total_cols[0]
                result = sum_values / group_vars[total_col] * 100
                # add a new column to the result dataframe with the result and the key as the column header
                self.processed_census_data[key] = result

            # if the length is exactly 2, you do not need to sum but you do need to divide over the corresponding total column
            elif len(group_vars.columns) == 2:
                total_col = total_cols[0]
                value_cols = [
                    col for col in group_vars.columns if col not in total_cols
                ]
                if (
                    "capita" in value_cols[0]
                ):  # if 'capita' is in the column name, do not multiply by 100
                    result = group_vars[value_cols].iloc[:, 0] / group_vars[total_col]
                else:
                    result = (
                        group_vars[value_cols].iloc[:, 0] / group_vars[total_col] * 100
                    )
                # add a new column to the result dataframe with the result and the key as the column header
                self.processed_census_data[key] = result

        self.processed_census_data["NAME"] = self.pd_census_data["NAME"]
        self.logger.info(
            "The specified variables are processed into their corresponding indicators, as specified in the Excel codebook"
        )

    def zscore(self, dataseries):
        return (dataseries - dataseries.mean()) / dataseries.std()

    def normalization_svi_data(self):
        """This function normalizes the processed indicator dataset using zscores for the first time
        The normalization direction is specified in the input excel codebook
        The inverse or normal standardization is based in the indicator's relation with social vulnerability
        If an indicator tends to increase social vulnerability, normal zscore is applied. If it decreases, the inverse is applied
        """
        selected_data = self.codebook[["Indicator_code", "zscore"]]
        selected_data.drop_duplicates(subset=["Indicator_code"], inplace=True)

        vulnerability_features = selected_data["Indicator_code"].tolist()
        direction = selected_data["zscore"].tolist()

        self.svi_fiat["NAME"] = self.processed_census_data["NAME"]
        svi_fiat_names = []

        for column, dir in zip(vulnerability_features, direction):
            if (
                column not in self.processed_census_data.columns
            ):  # If there are variables in the codebook but not in the data, this line ensures the model does not crash
                continue
            if dir == "normal":
                self.svi_fiat[column + "_norm"] = self.zscore(
                    self.processed_census_data[column]
                )
                svi_fiat_names.append(column + "_norm")
            elif dir == "inverse":
                self.svi_fiat[column + "_norm"] = (
                    self.zscore(self.processed_census_data[column]) * -1
                )
                svi_fiat_names.append(column + "_norm")
            else:
                self.logger.warning(
                    f"Normalization direction for {column} is not provided"
                )
        self.logger.info(
            "The data is standardized according to their relation specified in the codebook Excel (inverse or normal z-scores)"
        )

    def domain_scores(self):
        """This function groups the indicators together in domains (called 'Category' in the input excel codebook) and calculates their domain score
        First, the domaingroups need to contain the right indicators, specific to the dataset
        It could be that an indiator is missing because of variable exclusion on the basis of nan-criteria >5% and there are no variables left to compute an indicator
        In that case, the indicators are removed from domaingroups and the domain score is composed of the indicators that are there to make up a domain
        """

        self.pd_domain_scores = pd.DataFrame()

        # Get the column names from the processed dataframe
        columns_data = self.processed_census_data.columns.tolist()
        columns_data.remove("NAME")
        domaingroups = self.create_indicator_groups("Indicator_code", "Category")
        self.missing_indicators = []

        # Remove key-value pairs without a match -> you want to remove from the default match set (based in codebook) the variables that are no longer in your dataset
        if any(
            value not in columns_data
            for values in domaingroups.values()
            for value in values
        ):
            self.missing_indicators.append(
                [
                    value
                    for values in domaingroups.values()
                    for value in values
                    if value not in columns_data
                ]
            )
            # Create a new dictionary with the values that are present in columns_data
            domaingroups = {
                key: [value for value in values if value in columns_data]
                for key, values in domaingroups.items()
            }

        for key, values in domaingroups.items():
            # Add _norm to all values to match with the normalization column names
            values_norm = [v + "_norm" for v in values]
            sum_variables = self.svi_fiat.filter(
                items=values_norm
            )  # use the normalized dataset!
            z_variables = self.zscore(sum_variables)
            sum_values = z_variables.sum(axis=1) / len(
                values_norm
            )  # use the normalized dataset!
            self.pd_domain_scores[key] = sum_values
            self.pd_domain_scores_z[key] = sum_values

        # finding and storing the highest scoring domain for each row and adding it to the dataset
        self.pd_domain_scores_z["SVI_key_domain"] = self.pd_domain_scores_z.apply(
            lambda row: row.idxmax(), axis=1
        )

        # Now we add the names so we can later match these regions with their geometry
        self.pd_domain_scores_z["NAME"] = self.processed_census_data["NAME"]
        self.pd_domain_scores["NAME"] = self.processed_census_data["NAME"]

        self.logger.info(
            "The indicators are grouped together into their respective domains and are standardized again using zscores"
        )

    def composite_scores(self):
        """create composite scores from the domain scores"""
        columns_to_sum = self.pd_domain_scores_z.columns.drop(
            ["NAME", "SVI_key_domain"]
        )
        self.pd_domain_scores_z["composite_SVI"] = self.pd_domain_scores_z[
            columns_to_sum
        ].sum(axis=1)
        self.pd_domain_scores_z["composite_svi_z"] = self.zscore(
            self.pd_domain_scores_z["composite_SVI"]
        )

        self.logger.info(
            "Composite SVI scores are created based on the domain scores using equal weighting and are standardized again using zscores"
        )

    def match_geo_ID(self):
        """Matches GEO_IDs for the regions that are still in the dataset"""
        self.pd_domain_scores_geo = self.pd_domain_scores_z.copy()
        self.pd_domain_scores_geo["GEO_ID"] = (
            None  # Create a new column 'GEO_ID' with initial values set to None
        )

        for index, value in enumerate(self.pd_domain_scores_geo["NAME"]):
            if value in self.pd_census_data["NAME"].values:
                matching_row = self.pd_census_data.loc[
                    self.pd_census_data["NAME"] == value
                ]
                geo_id = matching_row["GEO_ID"].values[
                    0
                ]  # Assuming there's only one matching row, extract the GEO_ID value
                self.pd_domain_scores_geo.at[index, "GEO_ID"] = (
                    geo_id  # Assign the GEO_ID value to the corresponding row in self.pd_domain_scores_geo
                )
                self.pd_domain_scores_geo["GEOID_short"] = (
                    self.pd_domain_scores_geo["GEO_ID"].str.split("US").str[1]
                )

    def download_and_unzip(self, url, extract_to="."):
        """function to download the shapefile data from census tiger website

        Parameters
        ----------
        url : webpage
            URL to census website (TIGER) to download shapefiles for visualisation
        extract_to : str, optional
            _description_, by default '.'
        """

        try:
            http_response = urlopen(url)
            zipfile = ZipFile(BytesIO(http_response.read()))
            zipfile.extractall(path=extract_to)
        except Exception as e:
            self.logger.warning(f"Error during download and unzip: {e}")

    def download_shp_geom(self, year_data: int, counties: List[str]):
        """Downloading the shapefiles from the government Tiger website

        Parameters
        ----------
        year_data : int
            The year for which you want to download the census data and the corresponding shapefiles (for geometry)
        counties : List[str]
            A list of county codes in which your area of interest lies
        """
        block_groups_list = []
        for sf, county in zip_longest(
            self.state_fips, counties, fillvalue=self.state_fips[0]
        ):
            # Download shapefile of blocks
            if year_data == 2022:
                url = f"https://www2.census.gov/geo/tiger/TIGER_RD18/LAYER/FACES/tl_rd22_{sf}{county}_faces.zip"
            elif year_data == 2021:
                url = f"https://www2.census.gov/geo/tiger/TIGER2021/FACES/tl_2021_{sf}{county}_faces.zip"
            elif year_data == 2020:
                url = f"https://www2.census.gov/geo/tiger/TIGER2020PL/LAYER/FACES/tl_2020_{sf}{county}_faces.zip"
            else:
                self.logger.warning(
                    f"Year {year_data} not available from 'https://www2.census.gov/geo/tiger'"
                )
                return

            # Save shapefiles
            folder = (
                Path(self.save_folder) / "shapefiles" / (sf + county) / str(year_data)
            )
            self.logger.info(f"Downloading the county shapefile for {str(year_data)}")
            self.download_and_unzip(url, folder)
            shapefiles = list(Path(folder).glob("*.shp"))
            if shapefiles:
                shp = gpd.read_file(shapefiles[0])
                self.logger.info("The shapefile was downloaded")
            else:
                self.logger.warning(
                    f"No county shapefile found for county code {county} in state code {sf} on 'https://www2.census.gov/geo/tiger'. Continuing.."
                )
                continue

            # Dissolve shapefile based on block groups
            code = "20"
            attrs = ["STATEFP", "COUNTYFP", "TRACTCE", "BLKGRPCE"]
            attrs = [attr + code for attr in attrs]

            block_groups_shp = shp.dissolve(by=attrs, as_index=False)
            block_groups_shp = block_groups_shp[attrs + ["geometry"]]
            # block_groups["Census_Bg"] = block_groups['TRACTCE' + code].astype(str) + "-block" + block_groups['BLKGRPCE' + code].astype(str)
            block_groups_shp["GEO_ID"] = (
                "1500000US"
                + block_groups_shp["STATEFP" + code].astype(str)
                + block_groups_shp["COUNTYFP" + code].astype(str)
                + block_groups_shp["TRACTCE" + code].astype(str)
                + block_groups_shp["BLKGRPCE" + code].astype(str)
            )
            block_groups_list.append(block_groups_shp)

        self.block_groups = gpd.GeoDataFrame(pd.concat(block_groups_list))

        # NOTE: the shapefile downloaded from the census tiger website is deleted here!!
        # Delete the shapefile, that is not used anymore
        shp_folder = Path(self.save_folder) / "shapefiles"
        try:
            shutil.rmtree(shp_folder)
        except Exception as e:
            self.logger.warning(f"Folder {shp_folder} cannot be removed: {e}")

    def merge_svi_data_shp(self):
        """Merges the geometry data with the social vulnerability index computed in the module"""
        self.svi_data_shp = self.pd_domain_scores_geo.merge(
            self.block_groups[["GEO_ID", "geometry"]], on="GEO_ID", how="left"
        )
        self.svi_data_shp = gpd.GeoDataFrame(self.svi_data_shp)

        # Delete the rows that do not have a geometry column
        self.svi_data_shp = self.svi_data_shp.loc[
            self.svi_data_shp["geometry"].notnull()
        ]

        # self.svi_data_shp.drop(columns=columns_to_drop, inplace=True)
        self.svi_data_shp = self.svi_data_shp.to_crs(epsg=4326)
        self.logger.info(
            "The geometry information was successfully added to the social vulnerability information"
        )
