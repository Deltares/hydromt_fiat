import pandas as pd
from pandas.api.types import is_numeric_dtype
import numpy as np
import logging
from hydromt_fiat.workflows.utils import detect_delimiter
from typing import List


class Vulnerability:
    def __init__(
        self, 
        unit: str = "m",
    ):
        self.hazard_name = ""
        self.hazard_values = []
        self.functions = {}
        self.unit = unit

    def read(self, fn):
        """Read the vulnerability data."""
        csv_delimiter = detect_delimiter(fn)
        vulnerability_functions = pd.read_csv(fn, delimiter=csv_delimiter, engine="c")
        
        self.from_table(vulnerability_functions)
    
    def from_table(self, vulnerability_functions: pd.DataFrame):
        """Instantiate a Vulnerability class from a dataframe."""
        if vulnerability_functions.iloc[:, 0].name.startswith("#"):
            empty_header_rows = 0
        if vulnerability_functions.iloc[0, 0].startswith("#"):
            empty_header_rows = 1

        columns = list(vulnerability_functions.iloc[0 + empty_header_rows, :])
        damage_functions = vulnerability_functions.iloc[1 + empty_header_rows :, :]
        damage_functions.columns = columns
        damage_functions = damage_functions.astype(float)

        self.hazard_name = columns[0]
        self.hazard_values = list(damage_functions[self.hazard_name])

        self.unit = ""  # TODO read unit

        for c in columns[1:]:
            self.add(c, self.hazard_values, list(damage_functions[c]))

    def add(self, name: str, hazard_values: List[float], fraction_values: List[float]):
        """Add a vulnerability function."""
        if set(hazard_values) != set(self.hazard_values):
            self.update(name, hazard_values, fraction_values)
        else:
            self.functions[name] = fraction_values

    def add_multiple(
        self,
        names: List[str],
        hazard_values: List[float],
        fraction_values_list: List[List[float]],
    ):
        """Adds multiple vulnerability function."""
        for name, fraction_values in zip(names, fraction_values_list):
            self.add(name, hazard_values, fraction_values)

    def update(
        self, name: str, hazard_values: List[float], fraction_values: List[float]
    ):
        # Get the new hazard values of the existing and to be added damage functions
        _new_hazard_values = self.get_new_hazard_values(hazard_values)
        self.update_single_df(name, _new_hazard_values, fraction_values)

        other_damage_functions = self.get_damage_function_names()
        other_damage_functions.remove(name)

        for df_name in other_damage_functions:
            self.update_single_df(df_name, _new_hazard_values, self.functions[df_name])

        # Update the hazard_values variable
        self.hazard_values = _new_hazard_values

    def update_single_df(
        self, name: str, new_hazard_values: List[float], fraction_values: List[float]
    ):
        if len(fraction_values) == len(new_hazard_values):
            _new_fraction_values = fraction_values
        elif len(fraction_values) < len(new_hazard_values):
            # Tnterpolate the existing damage functions to the new damage function hazard values
            _new_fraction_values = self.interpolate_damage_function(
                new_hazard_values, fraction_values
            )

        # Add the vulnerability function to the functions variable
        self.functions[name] = _new_fraction_values

    def get_damage_function_names(self):
        return list(self.functions.keys())

    def check_vulnerability_source(self, df_vulnerability: pd.DataFrame()):
        # Load the vulnerability functions from the source
        # TODO: add some checks to validate the df_source
        return df_vulnerability

    def get_vulnerability_identifiers_and_linking_source(
        self, vulnerability_identifiers_and_linking_source: str
    ):
        # USER INPUT, read vulnerability identifiers and linking csv generated via gui or manual input.
        # TODO: add some checks to validate the df_identifiers_linking
        return pd.read_csv(vulnerability_identifiers_and_linking_source)

    def get_vulnerability_functions_from_one_file(
        self,
        df_source: pd.DataFrame,
        df_identifiers_linking: pd.DataFrame,
    ):
        # Identify the unique combinations of values from the identifiers and linking data frame that will be used to select subsets of values from the source data frame.
        # unique_combinations = df_identifiers_linking.groupby(['ID', 'Occupancy', 'Source', 'Description']).nunique().reset_index()

        identifier_columns = self.get_identifier_names(df_identifiers_linking)
        df_source = self.add_full_identifier_column(df_source, identifier_columns)
        df_identifiers_linking = self.add_full_identifier_column(
            df_identifiers_linking, identifier_columns
        )

        combined_df = self.link_vfs_from_source(df_source, df_identifiers_linking)

        # Delete the column used to link the dataframes
        del combined_df["full_identifier"]

        # Get the columns with damage fractions
        fraction_cols = self.get_hazard_value_columns(combined_df)

        # Get the hazard values from the columns
        hazard_values = self.get_hazard_values_from_columns(fraction_cols)

        # Get vulnerability factors
        vf_values_only = combined_df[fraction_cols].values

        # Check whether the vulnerability factors are fractions or percentages and
        # convert into fractions if they are percentages
        if vf_values_only.max() > 1:
            vf_values_only = vf_values_only / 100

        vf_names = df_identifiers_linking["Name"].values

        self.add_multiple(vf_names, hazard_values, vf_values_only)

    @staticmethod
    def get_identifier_names(
        df: pd.DataFrame, to_remove: list = ["Name", "Link"]
    ) -> list:
        """_summary_

        Parameters
        ----------
        df : pd.DataFrame
            _description_
        to_remove : list
            _description_

        Returns
        -------
        list
            _description_
        """
        # Check which columns are used as identifiers. These are all columns beside the 'to_remove' columns
        _identifier_columns = list(df.columns)
        for col in to_remove:
            _identifier_columns.remove(col)
        return _identifier_columns

    @staticmethod
    def add_full_identifier_column(
        df: pd.DataFrame, identifier_columns: list
    ) -> pd.DataFrame:
        # Create temporary columns of the identifier columns to be able to filter them
        # with flexible user input
        for c in identifier_columns:
            if is_numeric_dtype(df[c]):
                df[c] = df[c].astype(int).astype(str)

        df["full_identifier"] = df[identifier_columns].apply(
            lambda x: x.str.cat(sep=""), axis=1
        )
        df["full_identifier"] = df["full_identifier"].str.replace(" ", "")
        return df

    @staticmethod
    def link_vfs_from_source(
        vf_source: pd.DataFrame, vf_identifiers_linking: pd.DataFrame
    ) -> pd.DataFrame:
        # Initialize an empty list to hold the subsets
        subsets = []

        # Loop over the unique combinations of values
        for i in range(len(vf_identifiers_linking)):
            # Use the unique combination of values to select the corresponding subset of values
            # from the first data frame using boolean indexing
            subset = vf_source.loc[
                vf_source["full_identifier"]
                == vf_identifiers_linking.loc[i, "full_identifier"]
            ]

            # Check if the subset is empty
            if subset.empty:
                logging.warn(
                    f"No vulnerability curves found for unique combination {vf_identifiers_linking.loc[i]}"
                )

            # Append the subset of values to the list of subsets
            subsets.append(subset)

        # Concatenate all of the necessary vulnerability curves info into a single data frame using pd.concat()
        return pd.concat(subsets, ignore_index=True)

    @staticmethod
    def get_hazard_value_columns(df: pd.DataFrame):
        # Get the columns with damage fractions
        def has_numbers(inputString):
            return any(char.isdigit() for char in inputString)

        return [c for c in df.columns if has_numbers(c)]

    @staticmethod
    def get_hazard_values_from_columns(columns: list) -> list:
        # Get the hazard values from the column names
        list_hazard_values = list(
            map(
                lambda sub: float("".join([ele for ele in sub if ele.isnumeric()])),
                columns,
            )
        )
        if list_hazard_values[0] != 0:
            # The hazard values start with negative values
            list_hazard_values[: list_hazard_values.index(0)] = [
                -x for x in list_hazard_values[: list_hazard_values.index(0)]
            ]
        return list_hazard_values

    def truncate(self, damage_function_name: str, suffix: str, floodproof_to):
        truncate_to = floodproof_to + 0.01
        damfunc = self.functions[damage_function_name]
        new_df_name = damage_function_name + suffix

        if not (
            truncate_to in self.hazard_values and floodproof_to in self.hazard_values
        ):
            # Check if the hazard values already include the truncate_to and floodproof_to values
            closest_wd_idx = np.argsort(
                abs((np.array(self.hazard_values) - truncate_to))
            )[:1][0]
            line = pd.DataFrame(
                {self.hazard_name: truncate_to, damage_function_name: None},
                index=[closest_wd_idx],
            )
            damfunc = pd.concat(
                [
                    pd.DataFrame(
                        {
                            self.hazard_name: self.hazard_values[:closest_wd_idx],
                            damage_function_name: damfunc[:closest_wd_idx],
                        }
                    ),
                    line,
                    pd.DataFrame(
                        {
                            self.hazard_name: self.hazard_values[closest_wd_idx:],
                            damage_function_name: damfunc[closest_wd_idx:],
                        }
                    ),
                ]
            ).reset_index(drop=True)
            damfunc.set_index(self.hazard_name, inplace=True)
            damfunc.interpolate(method="index", axis=0, inplace=True)
            damfunc.reset_index(inplace=True)

            closest_wd_idx = np.argsort(
                abs((np.array(self.hazard_values) - floodproof_to))
            )[:2][1]
            line = pd.DataFrame(
                {self.hazard_name: floodproof_to, damage_function_name: 0.0},
                index=[closest_wd_idx],
            )
            damfunc = pd.concat(
                [
                    damfunc.iloc[:closest_wd_idx],
                    line,
                    damfunc.iloc[closest_wd_idx:],
                ]
            ).reset_index(drop=True)
            damfunc.loc[
                damfunc[self.hazard_name] < truncate_to, damage_function_name
            ] = 0.0

            # Save the truncated damage function to the damage functions variable
            self.add(
                new_df_name,
                list(damfunc[self.hazard_name]),
                list(damfunc[damage_function_name]),
            )
        else:
            damfunc = [
                val if haz >= truncate_to else 0
                for val, haz in zip(damfunc, self.hazard_values)
            ]
            self.add(
                new_df_name,
                self.hazard_values,
                damfunc,
            )

    def calculate_weighted_damage_function(
        self, damage_function_dict: dict, value_counts_dict: dict
    ):
        new_dfs = dict()

        for damage_type in damage_function_dict.keys():
            percentage_dfs = {
                idx: cnt / value_counts_dict[damage_type].sum()
                for cnt, idx in zip(
                    value_counts_dict[damage_type], value_counts_dict[damage_type].index
                )
            }
            print_percentage_dfs = {
                idx: str(round(cnt / value_counts_dict[damage_type].sum() * 100, 2))
                + "%"
                for cnt, idx in zip(
                    value_counts_dict[damage_type], value_counts_dict[damage_type].index
                )
            }
            logging.info(
                f"For the weighted {damage_type} damage function for the area where "
                "population growth will be accomodated, the following damage functions "
                f"are used with a percentual weight of: {print_percentage_dfs}"
            )

            new_damage_function = self.weighted_average_damage_function(
                damage_function_dict[damage_type],
                percentage_dfs,
            )
            new_damage_function_name = "new_composite_area_" + damage_type.lower()
            self.add(new_damage_function_name, self.hazard_values, new_damage_function)
            new_dfs[damage_type] = "new_development_area_" + damage_type.lower()

            logging.info(
                f"New damage function '{new_damage_function_name}' created for the new "
                "composite area."
            )

        return new_dfs

    def weighted_average_damage_function(
        self, dfs: List[str], percentage_dfs: dict
    ) -> List[float]:
        weighing = [percentage_dfs[df_id] for df_id in dfs]

        # Compute the weighted average of the damage fractions
        arrays = [np.array(self.functions[df_id]) for df_id in dfs]
        new_damage_fractions = np.average(np.array(arrays), axis=0, weights=weighing)

        return list(new_damage_fractions)

    def get_table(self):
        """Transforms the damage functions into a pandas DataFrame.

        Returns
        -------
        pd.DataFrame
            A DataFrame of vulnerability curves in the format that Delft-FIAT requires.
        """
        vf_values = np.array(list(self.functions.values())).T
        hazard_values = np.array(self.hazard_values).reshape(len(self.hazard_values), 1)
        vf_values = np.concatenate([hazard_values, vf_values], axis=1)

        vf_names_header = np.append(self.hazard_name, list(self.functions.keys()))
        top_header_array = np.full(
            (1, vf_names_header.shape[0]), fill_value="", dtype="<U100"
        )
        top_header_array[0, 0] = f"#UNIT={self.unit}"

        # TODO: add another header row for the method of aggregating the hazard values over an aereal object

        vf_names_header = vf_names_header.reshape(top_header_array.shape)

        vf_fiat_format = np.concatenate([top_header_array, vf_names_header, vf_values])

        # Create a dataframe out of the previous array.
        return pd.DataFrame(vf_fiat_format)

    def get_new_hazard_values(self, additional_hazard_values):
        all_hazard_values = additional_hazard_values + self.hazard_values
        all_hazard_values = list(set(all_hazard_values))
        all_hazard_values.sort()
        return all_hazard_values

    def interpolate_damage_function(
        self,
        new_hazard_values: list,
        fraction_values: list,
    ):
        hazard_value_column_name = "hazard_values"
        fractions_column_name = "fractions"

        df_output = pd.DataFrame(data={hazard_value_column_name: new_hazard_values})
        df_new = pd.DataFrame(
            {
                hazard_value_column_name: self.hazard_values,
                fractions_column_name: fraction_values,
            }
        )
        df_output = df_output.merge(
            df_new,
            how="outer",
            on=hazard_value_column_name,
        )

        # Sort the dataframe
        df_output.sort_values(hazard_value_column_name, inplace=True)

        # Fill the emtpy values in the dataframe.
        # First: interpolate to fill the nan values between values
        df_output.set_index(hazard_value_column_name, inplace=True)
        df_output.interpolate(method="index", limit_area="inside", inplace=True)

        # Second: fill the nan values before the first value per column (with 0)
        df_output.fillna(method="bfill", inplace=True)

        # Third: fill the nan values after the later value per column (with the same value as the last value)
        df_output.fillna(method="ffill", inplace=True)

        # Reset the index to a column
        df_output.reset_index(inplace=True)

        return list(df_output[fractions_column_name])
